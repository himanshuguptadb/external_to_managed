{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50cc4cc-5b80-4fd2-9520-c0836795bd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b5e8af6-0ec6-4de3-8a94-adad5b52bf16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def column_to_list(df, column):\n",
    "  df_pdf = df.toPandas()\n",
    "  df_list = df_pdf[column].tolist()\n",
    "  return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdcf5dc-55d9-4d9c-98ea-94271d031c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schemata(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting Schema owner and comments\")\n",
    "  schema_detail = spark.sql(f\"SELECT * FROM system.information_schema.schemata WHERE catalog_name =  '{catalog_name}' AND schema_name = '{schema_name}'\")\n",
    "  new_row = {'id': 1,'object_category': \"Schema\",'object_sub_category': \"Schema Definition\",'count': schema_detail.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return schema_detail,  external_schema_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6619c9f-41d5-4f1a-bcb8-5c8afe44383b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema_location(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting Schema location\")\n",
    "  schema_def = spark.sql(f\"DESCRIBE SCHEMA `{catalog_name}`.{schema_name}\")\n",
    "  schema_location = schema_def.filter(schema_def['database_description_item'] == 'Location') \n",
    "  new_row = {'id': 2,'object_category': \"Schema\",'object_sub_category': \"Schema Location\",'count': schema_location.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return schema_location, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd5eea8-760b-4b26-af3e-053ed11000dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema_tags(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting Schema tags\")\n",
    "  schema_tags = spark.sql(f\"SELECT * FROM system.information_schema.schema_tags WHERE catalog_name =  '{catalog_name}' AND schema_name = '{schema_name}'\")\n",
    "  new_row = {'id': 3,'object_category': \"Schema\",'object_sub_category': \"Schema Tags\",'count': schema_tags.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return schema_tags, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67784812-2338-45c3-8086-975e31d68728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema_grants(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting Schema grants\")\n",
    "  schema_grants = spark.sql(f\"SHOW GRANT ON SCHEMA `{catalog_name}`.{schema_name}\")\n",
    "  schema_grants = schema_grants.filter(schema_grants['ObjectType'] == 'SCHEMA')\n",
    "  new_row = {'id': 4,'object_category': \"Schema\",'object_sub_category': \"Schema Grants\",'count': schema_grants.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return schema_grants, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8732d425-2737-4591-bb26-4a2d1d1193c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unsupported_tables(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting unsupported tables name, description, type, format and owner\")\n",
    "  unsupported_tables = spark.sql(f\"SELECT table_name, table_type, data_source_format, table_owner, comment FROM system.information_schema.tables WHERE table_catalog =  '{catalog_name}' AND table_schema = '{schema_name}' and table_type IN ('FOREIGN','STREAMING_TABLE','MATERIALIZED_VIEW','MANAGED_SHALLOW_CLONE','EXTERNAL_SHALLOW_CLONE')\")\n",
    "  new_row = {'id': 5,'object_category': \"Tables & Views\",'object_sub_category': \"Unsupported Tables\",'count': unsupported_tables.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return unsupported_tables, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f97922-2282-45b5-939c-cbc6c02cad5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_supported_tables(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting Supported tables name, description, type, format and owner\")\n",
    "  tables = spark.sql(\n",
    "    f\"SELECT table_name, table_type, data_source_format, table_owner, comment FROM system.information_schema.tables WHERE table_catalog =  '{catalog_name}' AND table_schema = '{schema_name}' AND table_type in ('MANAGED','EXTERNAL')\")\n",
    "  new_row = {'id': 6,'object_category': \"Tables & Views\",'object_sub_category': \"Supported Tables\",'count': tables.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return tables, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a735091e-bd48-464d-b864-7a9a725a4c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_views(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting view details\")\n",
    "  view_details = spark.sql(\n",
    "    f\"SELECT v.*, t.table_owner, t.comment FROM system.information_schema.views v, system.information_schema.tables t WHERE t.table_catalog =  '{catalog_name}' AND t.table_schema = '{schema_name}' AND t.table_type = 'VIEW' AND v.table_name = t.table_name and v.table_schema = t.table_schema and v.table_catalog = t.table_catalog \")\n",
    "  new_row = {'id': 7,'object_category': \"Tables & Views\",'object_sub_category': \"Views\",'count': view_details.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return view_details, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59846087-35de-45e3-8175-4b7aa8d70c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_tables_views_tags(catalog_name, schema_name, external_schema_summary, table_names, view_names):\n",
    "  print(\"Getting tables and views tags\")\n",
    "  table_names_list = column_to_list(table_names,'table_name')\n",
    "  view_names_list = column_to_list(view_names,'table_name')\n",
    "  all_table_view_names_list = table_names_list + view_names_list\n",
    "  all_table_view_names_list = str(all_table_view_names_list)[1:-1]\n",
    "  tables_views_tags = spark.sql(\n",
    "    f\"SELECT * FROM system.information_schema.table_tags WHERE catalog_name =  '{catalog_name}' AND schema_name = '{schema_name}' AND table_name in ({all_table_view_names_list}) \")\n",
    "  new_row = {'id': 8,'object_category': \"Tables & Views\",'object_sub_category': \"Tags\",'count': tables_views_tags.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return tables_views_tags, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "983d38fd-e8d9-40f2-97f1-c4d357222529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_view_columns_comment(catalog_name, schema_name, external_schema_summary, table_names, view_names):\n",
    "  print(\"Getting tables and views columns comments\")\n",
    "  table_names_list = column_to_list(table_names,'table_name')\n",
    "  view_names_list = column_to_list(view_names,'table_name')\n",
    "  all_table_view_names_list = table_names_list + view_names_list\n",
    "  all_table_view_names_list = str(all_table_view_names_list)[1:-1]\n",
    "  table_view_columns_comment = spark.sql(\n",
    "    f\"SELECT table_catalog, table_schema, table_name, column_name, comment FROM system.information_schema.columns WHERE table_catalog =  '{catalog_name}' AND table_schema = '{schema_name}' AND table_name in ({all_table_view_names_list}) and comment is not null \")\n",
    "  new_row = {'id': 9,'object_category': \"Tables & Views\",'object_sub_category': \"Columns comment\",'count': table_view_columns_comment.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return table_view_columns_comment, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac027f9e-62db-421a-978f-00bcfe7a9de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_view_columns_tags(catalog_name, schema_name, external_schema_summary, table_names, view_names):\n",
    "  print(\"Getting tables and views column tags\")\n",
    "  table_names_list = column_to_list(table_names,'table_name')\n",
    "  view_names_list = column_to_list(view_names,'table_name')\n",
    "  all_table_view_names_list = table_names_list + view_names_list\n",
    "  all_table_view_names_list = str(all_table_view_names_list)[1:-1]\n",
    "  table_view_columns_tags = spark.sql(\n",
    "    f\"SELECT * FROM system.information_schema.column_tags WHERE catalog_name =  '{catalog_name}' AND schema_name = '{schema_name}' AND table_name in ({all_table_view_names_list}) and tag_name is not null \")\n",
    "  new_row = {'id': 10,'object_category': \"Tables & Views\",'object_sub_category': \"Columns tags\",'count': table_view_columns_tags.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return table_view_columns_tags, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef594629-803b-4297-8f5f-ddccc0ebbb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_view_constraints(catalog_name, schema_name, external_schema_summary, table_names):\n",
    "  print(\"Getting tables constraints\")\n",
    "  table_names_list = column_to_list(table_names,'table_name')\n",
    "  table_names_list = str(table_names_list)[1:-1]\n",
    "  table_constraints = spark.sql(\n",
    "    f\"SELECT * FROM system.information_schema.table_constraints WHERE table_catalog =  '{catalog_name}' AND table_schema = '{schema_name}' AND table_name in ({table_names_list}) \")\n",
    "  new_row = {'id': 11,'object_category': \"Tables & Views\",'object_sub_category': \"Constraints\",'count': table_constraints.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return table_constraints, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2101db0-e1c0-4838-8ffd-6a6c17a14341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_view_grants(catalog_name, schema_name, external_schema_summary, table_names, view_names):\n",
    "  print(\"Getting table adn view level grants\")\n",
    "  table_names_list = column_to_list(table_names,'table_name')\n",
    "  view_names_list = column_to_list(view_names,'table_name')\n",
    "  all_table_view_names_list = table_names_list + view_names_list\n",
    "  all_table_view_names_list = str(all_table_view_names_list)[1:-1]\n",
    "  all_table_view_grants = spark.sql(\n",
    "    f\"SELECT * FROM system.information_schema.table_privileges WHERE table_catalog =  '{catalog_name}' AND table_schema = '{schema_name}' AND table_name in ({all_table_view_names_list}) and inherited_from = 'NONE' \")\n",
    "  new_row = {'id': 12,'object_category': \"Tables & Views\",'object_sub_category': \"Grants\",'count': all_table_view_grants.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return all_table_view_grants, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132a654e-e03b-486b-906f-6d4d63860d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unsupported_volumes(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting unsupported volumes name, owner, location, description\")\n",
    "  unsupported_volumes = spark.sql(f\"SELECT * FROM system.information_schema.volumes WHERE volume_catalog =  '{catalog_name}' AND volume_schema = '{schema_name}' and volume_type IN ('MANAGED')\")\n",
    "  new_row = {'id': 13,'object_category': \"Volumes\",'object_sub_category': \"Unsupported Volumes\",'count': unsupported_volumes.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return unsupported_volumes, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d03852-5857-4b29-9927-9f878736265a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_supported_volumes(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting supported volumes name, owner, location, description\")\n",
    "  supported_volumes = spark.sql(f\"SELECT * FROM system.information_schema.volumes WHERE volume_catalog =  '{catalog_name}' AND volume_schema = '{schema_name}' and volume_type IN ('EXTERNAL')\")\n",
    "  new_row = {'id': 14,'object_category': \"Volumes\",'object_sub_category': \"Supported Volumes\",'count': supported_volumes.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return supported_volumes, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b44192c-1886-48dc-b5c4-259be8000441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_volume_tags(catalog_name, schema_name, external_schema_summary, volume_names):\n",
    "  print(\"Getting volumes tags\")\n",
    "  volumes_names_list = column_to_list(volume_names,'volume_name')\n",
    "  volumes_names_list = str(volumes_names_list)[1:-1]\n",
    "  volume_tags = spark.sql(\n",
    "    f\"SELECT * FROM system.information_schema.volume_tags WHERE catalog_name =  '{catalog_name}' AND schema_name = '{schema_name}' AND volume_name in ({volumes_names_list}) \")\n",
    "  new_row = {'id': 15,'object_category': \"Volumes\",'object_sub_category': \"Tags\",'count': volume_tags.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return volume_tags, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee960ae-ee58-4a85-8840-080ca5cea36c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_volume_grants(catalog_name, schema_name, external_schema_summary, volume_names):\n",
    "  print(\"Getting volume grants\")\n",
    "  volumes_names_list = column_to_list(volume_names,'volume_name')\n",
    "  volumes_names_list = str(volumes_names_list)[1:-1]\n",
    "  volume_grants = spark.sql(\n",
    "    f\"SELECT * FROM system.information_schema.volume_privileges WHERE volume_catalog =  '{catalog_name}' AND volume_schema = '{schema_name}' AND volume_name in ({volumes_names_list}) and inherited_from = 'NONE' \")\n",
    "  new_row = {'id': 16,'object_category': \"Volumes\",'object_sub_category': \"Grants\",'count': volume_grants.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return volume_grants, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47a771c9-a41e-49e1-9865-02b4d9d0fbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unsupported_functions(catalog_name, schema_name, external_schema_summary):\n",
    "  print(\"Getting unsupported functions name, owner, location, description\")\n",
    "  spark.sql(f\"use catalog `{catalog_name}`;\")\n",
    "  unsupported_functions = spark.sql(f\"SHOW USER FUNCTIONS IN {schema_name}\")\n",
    "  unsupported_functions = unsupported_functions.filter(unsupported_functions['function'] != 'getargument')\n",
    "  new_row = {'id': 17,'object_category': \"Functions\",'object_sub_category': \"Unsupported Functions\",'count': unsupported_functions.count()}\n",
    "  external_schema_summary = pd.concat([external_schema_summary, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return unsupported_functions, external_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa078e0-2c92-4e8d-a27b-518d4b6923a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_schema_scripts(catalog_name, schema_name, schema_root_location, schema_tags, schema_grants, schema_details):\n",
    "  schema_scripts = []\n",
    "  print(\"Creating managed schema script\")\n",
    "  schema_scripts.append(f\"CREATE SCHEMA IF NOT EXISTS `{catalog_name}`.{schema_name} MANAGED LOCATION '{schema_root_location}';\")\n",
    "  print(\"Creating schema tags script\")\n",
    "  for tags in schema_tags.collect():\n",
    "    schema_scripts.append(f\"ALTER SCHEMA `{catalog_name}`.{schema_name} SET TAGS ('{tags[2]}' = '{tags[3]}');\")\n",
    "  print(\"Creating schema grants script\")\n",
    "  for grant in schema_grants.collect():\n",
    "    schema_scripts.append(f\"GRANT {grant[1]} ON {grant[2]} `{catalog_name}`.{schema_name} TO `{grant[0]}`;\")\n",
    "  print(\"Creating schema comments and owner script\")\n",
    "  for schema in schema_details.collect():\n",
    "    schema_scripts.append(f\"COMMENT ON SCHEMA `{catalog_name}`.{schema_name} IS '{schema[3]}';\")\n",
    "    schema_scripts.append(f\"ALTER SCHEMA `{catalog_name}`.{schema_name} SET OWNER TO `{schema[2]}`;\")\n",
    "  return schema_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79b55c1-1091-4802-b31e-4dc2f95bd15e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_table_view_scripts(catalog_name, schema_name,backup_schema_name, tables, views, table_view_tags, column_comments, column_tags, table_view_grants ):\n",
    "  table_view_scripts = []\n",
    "  delta_tables = tables.filter(tables['DATA_SOURCE_FORMAT'] == 'DELTA')\n",
    "  non_delta_tables = tables.filter(tables['DATA_SOURCE_FORMAT'] != 'DELTA')\n",
    "  print(\"Creating delta clone table and cluster by script \")\n",
    "  for table in delta_tables.collect():\n",
    "    table_view_scripts.append(f\"CREATE OR REPLACE TABLE `{catalog_name}`.{schema_name}.{table[0]} CLONE `{catalog_name}`.{backup_schema_name}.{table[0]};\")\n",
    "    table_view_scripts.append(f\"ALTER TABLE `{catalog_name}`.{schema_name}.{table[0]} CLUSTER BY AUTO;\")\n",
    "\n",
    "  print(\"Creating drop and create non delta external table script\")\n",
    "  for table in non_delta_tables.collect():\n",
    "    table_view_scripts.append(f\"DROP TABLE `{catalog_name}`.{backup_schema_name}.{table[0]};\")\n",
    "    table_view_scripts.append(spark.sql(f\"SHOW CREATE TABLE `{catalog_name}`.{schema_name}.{table[0]}\").collect()[0][0])\n",
    "\n",
    "  print(\"Creating view script\")\n",
    "  for view in views.collect():\n",
    "    table_view_scripts.append(f\"CREATE OR REPLACE VIEW `{catalog_name}`.{schema_name}.{view[2]} AS {view[3]};\")\n",
    "  \n",
    "  print(\"Creating table tags script\")\n",
    "  table_tags = table_view_tags.join(tables, tables['table_name'] == table_view_tags['table_name'], 'inner')\n",
    "  for tags in table_tags.collect():\n",
    "    table_view_scripts.append(f\"ALTER TABLE `{catalog_name}`.{schema_name}.{tags[2]} SET TAGS ('{tags[3]}' = '{tags[4]}');\")\n",
    "  \n",
    "  print(\"Creating view tags script\")\n",
    "  view_tags = table_view_tags.join(views, views['table_name'] == table_view_tags['table_name'], 'inner')\n",
    "  for tags in view_tags.collect():\n",
    "    table_view_scripts.append(f\"ALTER VIEW `{catalog_name}`.{schema_name}.{tags[2]} SET TAGS ('{tags[3]}' = '{tags[4]}');\")\n",
    "\n",
    "  #print(\"Creating table and view column comments script\")\n",
    "  #for comment in column_comments.collect():\n",
    "    #table_view_scripts.append(f\"COMMENT ON COLUMN `{catalog_name}`.{schema_name}.{comment[2]}.{comment[3]} IS '{comment[4]}';\")\n",
    "  \n",
    "  print(\"Creating table and views column tags script\")\n",
    "  table_column_tags = column_tags.join(tables, tables['table_name'] == column_tags['table_name'], 'inner')\n",
    "  for tags in table_column_tags.collect():\n",
    "    table_view_scripts.append(f\"ALTER TABLE `{catalog_name}`.{schema_name}.{tags[2]} ALTER COLUMN {tags[3]} SET TAGS ('{tags[4]}' = '{tags[5]}');\")\n",
    "  view_column_tags = column_tags.join(views, views['table_name'] == column_tags['table_name'], 'inner')\n",
    "  for tags in view_column_tags.collect():\n",
    "    table_view_scripts.append(f\"ALTER TABLE `{catalog_name}`.{schema_name}.{tags[2]} ALTER COLUMN {tags[3]} SET TAGS ('{tags[4]}' = '{tags[5]}');\")\n",
    "\n",
    "  print(\"Creating table grants script\")\n",
    "  table_grants = table_view_grants.join(tables, tables['table_name'] == table_view_grants['table_name'], 'inner')\n",
    "  for grant in table_grants.collect():\n",
    "    table_view_scripts.append(f\"GRANT {grant[5]} ON TABLE `{grant[2]}`.{grant[3]}.{grant[4]} TO `{grant[1]}`;\")\n",
    "  \n",
    "  print(\"Creating view grants script\")\n",
    "  view_grants = table_view_grants.join(views, views['table_name'] == table_view_grants['table_name'], 'inner')\n",
    "  for grant in table_grants.collect():\n",
    "    table_view_scripts.append(f\"GRANT {grant[5]} ON VIEW `{grant[2]}`.{grant[3]}.{grant[4]} TO `{grant[1]}`;\")\n",
    "  \n",
    "  print(\"Creating table owner and comment script\")\n",
    "  for table in tables.collect():\n",
    "    #table_view_scripts.append(f\"COMMENT ON TABLE `{catalog_name}`.{schema_name}.{table[0]} IS '{table[4]}';\")\n",
    "    table_view_scripts.append(f\"ALTER TABLE `{catalog_name}`.{schema_name}.{table[0]} SET OWNER TO `{table[3]}`;\")\n",
    "  \n",
    "  print(\"Creating view owner and comment script\")\n",
    "  for view in views.collect():\n",
    "    #table_view_scripts.append(f\"COMMENT ON VIEW `{catalog_name}`.{schema_name}.{view[2]} IS '{view[10]}';\")\n",
    "    table_view_scripts.append(f\"ALTER VIEW `{catalog_name}`.{schema_name}.{view[2]} SET OWNER TO `{view[9]}`;\")\n",
    "\n",
    "  return table_view_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8eebbce-9b3f-40e9-852a-8c954aa494c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_volume_scripts(catalog_name, schema_name,backup_schema_name, volumes, volume_tags, volume_grants ):\n",
    "  volume_scripts =[]\n",
    "  print(\"Creating drop create volume with comments and owner script\")\n",
    "  for volume in volumes.collect():\n",
    "    volume_scripts.append(f\"DROP VOLUME `{catalog_name}`.{backup_schema_name}.{volume[2]};\")\n",
    "    volume_scripts.append(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS `{catalog_name}`.{schema_name}.{volume[2]} LOCATION '{volume[6]}';\")\n",
    "    volume_scripts.append(f\"COMMENT ON VOLUME `{catalog_name}`.{schema_name}.{volume[2]} IS '{volume[5]}';\")\n",
    "    volume_scripts.append(f\"ALTER VOLUME `{catalog_name}`.{schema_name}.{volume[2]} SET OWNER TO `{volume[4]}`;\")\n",
    "  \n",
    "  print(\"Creating volume tags script\")\n",
    "  for tags in volume_tags.collect():\n",
    "    volume_scripts.append(f\"ALTER VOLUME `{catalog_name}`.{schema_name}.{tags[2]} SET TAGS ('{tags[3]}' = '{tags[4]}');\")\n",
    "\n",
    "  print(\"Creating volume grants script\")\n",
    "  for grant in volume_grants.collect():\n",
    "    volume_scripts.append(f\"GRANT {grant[5]} ON VOLUME `{grant[2]}`.{grant[3]}.{grant[4]} TO `{grant[1]}`;\")\n",
    "  return volume_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b6caae-fb4f-4ae1-96a0-00ca82995d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_files(path: str):\n",
    "    total_files = 0\n",
    "    items = dbutils.fs.ls(path)\n",
    "    for item in items:\n",
    "        if item.isDir():\n",
    "            total_files += count_files(item.path)\n",
    "        else:\n",
    "            total_files += 1\n",
    "    return total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191a1a23-bfe3-453f-93b1-480e1afb6baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_count(catalog_name, schema_name, tables, views, volumes):\n",
    "  data_count = pd.DataFrame(columns=[\"catalog_name\",\"schema_name\", \"object_name\", \"count\"])\n",
    "  for table in tables.collect():\n",
    "    size = spark.sql(f\"SELECT count(*) FROM `{catalog_name}`.{schema_name}.{table[0]}\").collect()[0][0]\n",
    "    new_row = {'catalog_name': f\"{catalog_name}\",'schema_name': f\"{schema_name}\",'object_name': f\"{table[0]}\",'count': size}\n",
    "    data_count = pd.concat([data_count, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  for view in views.collect():\n",
    "    size = spark.sql(f\"SELECT count(*) FROM `{catalog_name}`.{schema_name}.{view[2]}\").collect()[0][0]\n",
    "    new_row = {'catalog_name': f\"{catalog_name}\",'schema_name': f\"{schema_name}\",'object_name': f\"{view[2]}\",'count': size}\n",
    "    data_count = pd.concat([data_count, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  for volume in volumes.collect():\n",
    "    path = f\"/Volumes/{volume[0]}/{volume[1]}/{volume[2]}/\"\n",
    "    total_files = 0\n",
    "    items = dbutils.fs.ls(path)\n",
    "    for item in items:\n",
    "        if item.isDir():\n",
    "            total_files += count_files(item.path)\n",
    "        else:\n",
    "            total_files += 1\n",
    "    new_row = {'catalog_name': f\"{catalog_name}\",'schema_name': f\"{schema_name}\",'object_name': f\"{volume[2]}\",'count': total_files}\n",
    "    data_count = pd.concat([data_count, pd.DataFrame([new_row])], ignore_index=True)\n",
    "  return data_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9732a8d-2fd2-4dbe-a686-eb761e98484b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_prefix_to_columns(df, prefix):\n",
    "    df.columns = [f\"{prefix}{col}\" for col in df.columns]\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
